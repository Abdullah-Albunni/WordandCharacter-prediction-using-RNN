{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8449954,"sourceType":"datasetVersion","datasetId":5035558}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport warnings\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='tensorflow')\n\n# Read the text file\nwith open('/kaggle/input/sportss/sports.txt', 'r', encoding='utf-8') as file:\n    text = file.read()\n\n# Word-level Tokenization and Model\nword_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts([text])\n\n# Convert text to sequences of words\nword_encoded = word_tokenizer.texts_to_sequences([text])[0]\n\n# Create input sequences using list of n-gram sequences\ninput_seq_word = []\nfor i in range(1, len(word_encoded)):\n    n_gram_sequence = word_encoded[:i+1]\n    input_seq_word.append(n_gram_sequence)\n\n# Pad sequences\nmax_sequence_len_word = max([len(seq) for seq in input_seq_word])\ninput_sequences_word = pad_sequences(input_seq_word, maxlen=max_sequence_len_word, padding='pre')\n\n# Split input sequences into x and y\nx_word = input_sequences_word[:, :-1]\ny_word = input_sequences_word[:, -1]\n\n# One-hot encode the labels\ntotal_words = len(word_tokenizer.word_index) + 1\ny_word = tf.keras.utils.to_categorical(y_word, num_classes=total_words)\n\n# Define the word-level model\nword_model = Sequential()\nword_model.add(Embedding(total_words, 100, input_length=max_sequence_len_word-1))\nword_model.add(SimpleRNN(150))\nword_model.add(Dense(total_words, activation='softmax'))\n\n# Build the model\nword_model.build(input_shape=(None, max_sequence_len_word-1))\nprint(word_model.summary())\n\n# Compile the model\nword_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the word-level model\nword_model.fit(x_word, y_word, epochs=100, verbose=1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T16:07:11.667105Z","iopub.execute_input":"2024-05-18T16:07:11.667935Z","iopub.status.idle":"2024-05-18T16:54:23.752145Z","shell.execute_reply.started":"2024-05-18T16:07:11.667894Z","shell.execute_reply":"2024-05-18T16:54:23.751145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Character-level Tokenization and Model\nchar_tokenizer = Tokenizer(char_level=True)\nchar_tokenizer.fit_on_texts([text])\n\n# Convert text to sequences of characters\nchar_encoded = char_tokenizer.texts_to_sequences([text])[0]\n\n# Create input sequences using list of n-gram sequences\ninput_seq_char = []\nfor i in range(1, len(char_encoded)):\n    n_gram_sequence = char_encoded[:i+1]\n    input_seq_char.append(n_gram_sequence)\n\n# Pad sequences\nmax_sequence_len_char = max([len(seq) for seq in input_seq_char])\ninput_sequences_char = pad_sequences(input_seq_char, maxlen=max_sequence_len_char, padding='pre')\n\n# Split input sequences into x and y\nx_char = input_sequences_char[:, :-1]\ny_char = input_sequences_char[:, -1]\n\n# One-hot encode the labels\ntotal_chars = len(char_tokenizer.word_index) + 1\ny_char = tf.keras.utils.to_categorical(y_char, num_classes=total_chars)\n\n# Define the character-level model\nchar_model = Sequential()\nchar_model.add(Embedding(total_chars, 50, input_length=max_sequence_len_char-1))\nchar_model.add(SimpleRNN(100))\nchar_model.add(Dense(total_chars, activation='softmax'))\n\n# Build the model\nchar_model.build(input_shape=(None, max_sequence_len_char-1))\nprint(char_model.summary())\n\n# Compile the model\nchar_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the character-level model\nchar_model.fit(x_char, y_char, epochs=10, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:57:21.683404Z","iopub.execute_input":"2024-05-18T16:57:21.683772Z","iopub.status.idle":"2024-05-18T19:36:28.206947Z","shell.execute_reply.started":"2024-05-18T16:57:21.683744Z","shell.execute_reply":"2024-05-18T19:36:28.206005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to predict the next characters\ndef predict_next_chars(input_text, num_chars):\n    for _ in range(num_chars):\n        token_list = char_tokenizer.texts_to_sequences([input_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len_char-1, padding='pre')\n        predicted = np.argmax(char_model.predict(token_list), axis=-1)\n        output_char = \"\"\n        for char, index in char_tokenizer.word_index.items():\n            if index == predicted:\n                output_char = char\n                break\n        input_text += output_char\n    return input_text\n\n\n\n# Function to predict the next words\ndef predict_next_words(input_text, num_words):\n    for _ in range(num_words):\n        token_list = word_tokenizer.texts_to_sequences([input_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len_word-1, padding='pre')\n        predicted = np.argmax(word_model.predict(token_list), axis=-1)\n        output_word = \"\"\n        for word, index in word_tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        input_text += \" \" + output_word\n    return input_text","metadata":{"execution":{"iopub.status.busy":"2024-05-18T20:12:33.817208Z","iopub.execute_input":"2024-05-18T20:12:33.817559Z","iopub.status.idle":"2024-05-18T20:12:33.826511Z","shell.execute_reply.started":"2024-05-18T20:12:33.817532Z","shell.execute_reply":"2024-05-18T20:12:33.825598Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Test the character-level model\ninput_text_char = \"Sp\"\nnum_predict_chars = int(input(\"Enter the number of characters to predict: \"))\noutput_text_char = predict_next_chars(input_text_char, num_predict_chars)\nprint(\"Predicted next characters:\", output_text_char)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T20:12:36.678065Z","iopub.execute_input":"2024-05-18T20:12:36.678434Z","iopub.status.idle":"2024-05-18T20:12:47.307361Z","shell.execute_reply.started":"2024-05-18T20:12:36.678405Z","shell.execute_reply":"2024-05-18T20:12:47.306455Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the number of characters to predict:  4\n"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\nPredicted next characters: Sports\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test the word-level model\ninput_text_word = \"activities with different\"\nnum_predict_words = int(input(\"Enter the number of words to predict: \"))\noutput_text_word = predict_next_words(input_text_word, num_predict_words)\nprint(\"Predicted next words:\", output_text_word)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T20:12:49.390444Z","iopub.execute_input":"2024-05-18T20:12:49.390813Z","iopub.status.idle":"2024-05-18T20:12:53.393459Z","shell.execute_reply.started":"2024-05-18T20:12:49.390785Z","shell.execute_reply":"2024-05-18T20:12:53.392495Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the number of words to predict:  15\n"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\nPredicted next words: activities with different names according to the ways of playing them sports are generally liked by almost all\n","output_type":"stream"}]}]}